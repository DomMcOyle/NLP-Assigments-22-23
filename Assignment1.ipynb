{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DomMcOyle/NLP-Assigments-22-23/blob/Assignment-1/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WeCeITXoxLf"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "**Due to**: 11/01/2022 (dd/mm/yyyy)\n",
        "\n",
        "If you deliver it by 11/12/2021 your assignment will be graded by 11/01/2022.\n",
        "\n",
        "\n",
        "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
        "\n",
        "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures\n",
        "\n",
        "Please, read ALL the following instructions :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4_wqPdlBcKS"
      },
      "source": [
        "# Intro\n",
        "\n",
        "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
        "\n",
        "You are asked to follow these steps:\n",
        "*   Download the corpora and split it in training and test sets, structuring a dataframe.\n",
        "*   Embed the words using GloVe embeddings\n",
        "*   Create a baseline model, using a simple neural architecture\n",
        "*   Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
        "*   Evaluate your two best model\n",
        "*   Analyze the errors of your model\n",
        "\n",
        "\n",
        "**Task**: given a corpus of documents, predict the POS tag for each word\n",
        "\n",
        "**Corpus**:\n",
        "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
        "The corpus is available at:\n",
        "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "\n",
        "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
        "\n",
        "\n",
        "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
        "\n",
        "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
        "\n",
        "**I/O structure**: The input data will have three dimensions: 1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
        "\n",
        "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
        "\n",
        "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
        "\n",
        "\n",
        "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
        "\n",
        "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES. [What is punctuation?]{https://en.wikipedia.org/wiki/English_punctuation}\n",
        "\n",
        "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
        "\n",
        "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
        "\n",
        "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
        "\n",
        "# Out Of Vocabulary (OOV) terms\n",
        "\n",
        "How to handle words that are not in GloVe vocabulary?\n",
        "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
        "\n",
        "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
        "If you use random embeddings or placeholder embeddings (e.g., everything is 0) this is automatically solved.\n",
        "\n",
        "If you want to use other techniques: when you compute new embeddings for train+validation, you must forget about test documents.\n",
        "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
        "\n",
        "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
        "\n",
        "Basically in a real-world scenario, this is what would happen:\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Training of the model(s)\n",
        "5. Compute embeddings for terms OOV2 of the validation split \n",
        "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "7. Validation of the model(s)\n",
        "8. Compute embeddings for terms OOV3 of the test split \n",
        "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "10. Testing of the final model\n",
        "\n",
        "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
        "\n",
        "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
        "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
        "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
        "4. Compute embeddings for terms OOV2 of the validation split \n",
        "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
        "6. Compute embeddings for terms OOV3 of the test split \n",
        "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
        "8. Training of the model(s)\n",
        "9. Validation of the model(s)\n",
        "10. Testing of the final model\n",
        "\n",
        "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
        "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
        "\n",
        "# Report\n",
        "The report must not be just a copy and paste of graphs and tables!\n",
        "\n",
        "The report must not be longer than 2 pages and must contain:\n",
        "* The names of the member of your team\n",
        "* A short abstract (4-5 lines) that sum ups everything\n",
        "* A general (brief!) description of the task you have addressed and how you have addressed it\n",
        "* A brief analysis of the data (class distribution, lenght, etc)\n",
        "* A short description of the models you have used\n",
        "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
        "* The most relevant findings of your error analysis\n",
        "\n",
        "\n",
        "# Evaluation Criterion\n",
        "\n",
        "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
        "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
        "\n",
        "The score of the assignment will be computed roughly as follows\n",
        "* 1 point for the correctness of approach/methodology\n",
        "* 1 point for the handling of OOV terms\n",
        "* 1 point for the correct implementation of models (they must run and do the right thing)\n",
        "* 1 point for train-validation-test procedure\n",
        "* 2 point for the discussion of the results, error analysis, and report\n",
        "\n",
        "The evaluation will not be based on the performance of the models!\n",
        "\n",
        "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. \n",
        "\n",
        "# Deliver\n",
        "\n",
        "* Two files: a pdf file for the report, and a python notebook\n",
        "* Put the names of the member of the team at the beginning of both files\n",
        "* What about additional files, for example models or weights? You can upload them in a private cloud and insert the link in the report.\n",
        "* Please, submit a notebook with clear sections, text boxes, and comments and indications about what is going on in the code. When we do not understand what is going on we get annoyed, when we are annoyed we tend to be more severe in our evaluation.\n",
        "\n",
        "\n",
        "\n",
        "# Contacts\n",
        "\n",
        "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1273064/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
        "\n",
        "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
        "If we do not reply within 3-4 days, please send it again!\n",
        "\n",
        "Teaching Assistants:\n",
        "\n",
        "* Andrea Galassi -> a.galassi@unibo.it\n",
        "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
        "\n",
        "Professor:\n",
        "\n",
        "* Paolo Torroni -> p.torroni@unibo.it\n",
        "\n",
        "\n",
        "# Additional notes and FAQ\n",
        "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
        "* For the baseline, it must have only two trainable layers: the BiLSTM and the Dense/FC one. The Dense layer is the \"classification head\" with softmax activation. You must not add an additional dense layer on top of the baseline. You can use the embedding layer before the BiLSTM, but it must be not trainable.\n",
        "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them.\n",
        "* For the application of the Dense Layer, it is recommended to use a Time-Distributed Dense. In any case, doing otherwise is NOT considered an error.\n",
        "* Some examples of things you can analyze in your discussion and error analysis are: the performances on the most frequent classes and the less frequent classes, precision and recall, the confusion tables, specific misclassified samples.\n",
        "* Punctuation: you must keep the punctuation in the documents, since it may be helpful for the model, you simply must ignore it when you perform the evaluation of the model, not considering the punctuation classes among the ones you use to compute F1 macro score. If you are curious, you can run ADDITIONAL experiments where you remove punctuation to see its impact."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1"
      ],
      "metadata": {
        "id": "AP1iyYTTY3gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
        "!unzip dependency_treebank.zip\n",
        "!rm dependency_treebank.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcrFcorDYzX1",
        "outputId": "b83cf05b-bd2e-40d2-f478-f07b4b927039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-17 19:25:45--  https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 457429 (447K) [application/zip]\n",
            "Saving to: ‘dependency_treebank.zip’\n",
            "\n",
            "dependency_treebank 100%[===================>] 446.71K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-11-17 19:25:45 (34.6 MB/s) - ‘dependency_treebank.zip’ saved [457429/457429]\n",
            "\n",
            "Archive:  dependency_treebank.zip\n",
            "   creating: dependency_treebank/\n",
            "  inflating: dependency_treebank/wsj_0093.dp  \n",
            "  inflating: dependency_treebank/wsj_0065.dp  \n",
            "  inflating: dependency_treebank/wsj_0039.dp  \n",
            "  inflating: dependency_treebank/wsj_0182.dp  \n",
            "  inflating: dependency_treebank/wsj_0186.dp  \n",
            "  inflating: dependency_treebank/wsj_0041.dp  \n",
            "  inflating: dependency_treebank/wsj_0018.dp  \n",
            "  inflating: dependency_treebank/wsj_0105.dp  \n",
            "  inflating: dependency_treebank/wsj_0149.dp  \n",
            "  inflating: dependency_treebank/wsj_0194.dp  \n",
            "  inflating: dependency_treebank/wsj_0055.dp  \n",
            "  inflating: dependency_treebank/wsj_0187.dp  \n",
            "  inflating: dependency_treebank/wsj_0143.dp  \n",
            "  inflating: dependency_treebank/wsj_0052.dp  \n",
            "  inflating: dependency_treebank/wsj_0064.dp  \n",
            "  inflating: dependency_treebank/wsj_0179.dp  \n",
            "  inflating: dependency_treebank/wsj_0195.dp  \n",
            "  inflating: dependency_treebank/wsj_0051.dp  \n",
            "  inflating: dependency_treebank/wsj_0059.dp  \n",
            "  inflating: dependency_treebank/wsj_0109.dp  \n",
            "  inflating: dependency_treebank/wsj_0074.dp  \n",
            "  inflating: dependency_treebank/wsj_0089.dp  \n",
            "  inflating: dependency_treebank/wsj_0108.dp  \n",
            "  inflating: dependency_treebank/wsj_0104.dp  \n",
            "  inflating: dependency_treebank/wsj_0164.dp  \n",
            "  inflating: dependency_treebank/wsj_0024.dp  \n",
            "  inflating: dependency_treebank/wsj_0008.dp  \n",
            "  inflating: dependency_treebank/wsj_0101.dp  \n",
            "  inflating: dependency_treebank/wsj_0132.dp  \n",
            "  inflating: dependency_treebank/wsj_0028.dp  \n",
            "  inflating: dependency_treebank/wsj_0184.dp  \n",
            "  inflating: dependency_treebank/wsj_0082.dp  \n",
            "  inflating: dependency_treebank/wsj_0114.dp  \n",
            "  inflating: dependency_treebank/wsj_0061.dp  \n",
            "  inflating: dependency_treebank/wsj_0190.dp  \n",
            "  inflating: dependency_treebank/wsj_0034.dp  \n",
            "  inflating: dependency_treebank/wsj_0043.dp  \n",
            "  inflating: dependency_treebank/wsj_0044.dp  \n",
            "  inflating: dependency_treebank/wsj_0021.dp  \n",
            "  inflating: dependency_treebank/wsj_0005.dp  \n",
            "  inflating: dependency_treebank/wsj_0112.dp  \n",
            "  inflating: dependency_treebank/wsj_0167.dp  \n",
            "  inflating: dependency_treebank/wsj_0042.dp  \n",
            "  inflating: dependency_treebank/wsj_0168.dp  \n",
            "  inflating: dependency_treebank/wsj_0185.dp  \n",
            "  inflating: dependency_treebank/wsj_0057.dp  \n",
            "  inflating: dependency_treebank/wsj_0015.dp  \n",
            "  inflating: dependency_treebank/wsj_0116.dp  \n",
            "  inflating: dependency_treebank/wsj_0135.dp  \n",
            "  inflating: dependency_treebank/wsj_0175.dp  \n",
            "  inflating: dependency_treebank/wsj_0171.dp  \n",
            "  inflating: dependency_treebank/wsj_0068.dp  \n",
            "  inflating: dependency_treebank/wsj_0080.dp  \n",
            "  inflating: dependency_treebank/wsj_0035.dp  \n",
            "  inflating: dependency_treebank/wsj_0181.dp  \n",
            "  inflating: dependency_treebank/wsj_0177.dp  \n",
            "  inflating: dependency_treebank/wsj_0102.dp  \n",
            "  inflating: dependency_treebank/wsj_0137.dp  \n",
            "  inflating: dependency_treebank/wsj_0022.dp  \n",
            "  inflating: dependency_treebank/wsj_0176.dp  \n",
            "  inflating: dependency_treebank/wsj_0180.dp  \n",
            "  inflating: dependency_treebank/wsj_0121.dp  \n",
            "  inflating: dependency_treebank/wsj_0128.dp  \n",
            "  inflating: dependency_treebank/wsj_0036.dp  \n",
            "  inflating: dependency_treebank/wsj_0071.dp  \n",
            "  inflating: dependency_treebank/wsj_0091.dp  \n",
            "  inflating: dependency_treebank/wsj_0076.dp  \n",
            "  inflating: dependency_treebank/wsj_0123.dp  \n",
            "  inflating: dependency_treebank/wsj_0075.dp  \n",
            "  inflating: dependency_treebank/wsj_0131.dp  \n",
            "  inflating: dependency_treebank/wsj_0050.dp  \n",
            "  inflating: dependency_treebank/wsj_0136.dp  \n",
            "  inflating: dependency_treebank/wsj_0161.dp  \n",
            "  inflating: dependency_treebank/wsj_0033.dp  \n",
            "  inflating: dependency_treebank/wsj_0188.dp  \n",
            "  inflating: dependency_treebank/wsj_0085.dp  \n",
            "  inflating: dependency_treebank/wsj_0014.dp  \n",
            "  inflating: dependency_treebank/wsj_0073.dp  \n",
            "  inflating: dependency_treebank/wsj_0199.dp  \n",
            "  inflating: dependency_treebank/wsj_0120.dp  \n",
            "  inflating: dependency_treebank/wsj_0178.dp  \n",
            "  inflating: dependency_treebank/wsj_0122.dp  \n",
            "  inflating: dependency_treebank/wsj_0040.dp  \n",
            "  inflating: dependency_treebank/wsj_0020.dp  \n",
            "  inflating: dependency_treebank/wsj_0153.dp  \n",
            "  inflating: dependency_treebank/wsj_0107.dp  \n",
            "  inflating: dependency_treebank/wsj_0017.dp  \n",
            "  inflating: dependency_treebank/wsj_0140.dp  \n",
            "  inflating: dependency_treebank/wsj_0038.dp  \n",
            "  inflating: dependency_treebank/wsj_0031.dp  \n",
            "  inflating: dependency_treebank/wsj_0165.dp  \n",
            "  inflating: dependency_treebank/wsj_0146.dp  \n",
            "  inflating: dependency_treebank/wsj_0090.dp  \n",
            "  inflating: dependency_treebank/wsj_0001.dp  \n",
            "  inflating: dependency_treebank/wsj_0148.dp  \n",
            "  inflating: dependency_treebank/wsj_0097.dp  \n",
            "  inflating: dependency_treebank/wsj_0009.dp  \n",
            "  inflating: dependency_treebank/wsj_0173.dp  \n",
            "  inflating: dependency_treebank/wsj_0111.dp  \n",
            "  inflating: dependency_treebank/wsj_0129.dp  \n",
            "  inflating: dependency_treebank/wsj_0130.dp  \n",
            "  inflating: dependency_treebank/wsj_0047.dp  \n",
            "  inflating: dependency_treebank/wsj_0110.dp  \n",
            "  inflating: dependency_treebank/wsj_0113.dp  \n",
            "  inflating: dependency_treebank/wsj_0147.dp  \n",
            "  inflating: dependency_treebank/wsj_0160.dp  \n",
            "  inflating: dependency_treebank/wsj_0099.dp  \n",
            "  inflating: dependency_treebank/wsj_0003.dp  \n",
            "  inflating: dependency_treebank/wsj_0011.dp  \n",
            "  inflating: dependency_treebank/wsj_0056.dp  \n",
            "  inflating: dependency_treebank/wsj_0069.dp  \n",
            "  inflating: dependency_treebank/wsj_0026.dp  \n",
            "  inflating: dependency_treebank/wsj_0138.dp  \n",
            "  inflating: dependency_treebank/wsj_0029.dp  \n",
            "  inflating: dependency_treebank/wsj_0115.dp  \n",
            "  inflating: dependency_treebank/wsj_0037.dp  \n",
            "  inflating: dependency_treebank/wsj_0019.dp  \n",
            "  inflating: dependency_treebank/wsj_0002.dp  \n",
            "  inflating: dependency_treebank/wsj_0007.dp  \n",
            "  inflating: dependency_treebank/wsj_0158.dp  \n",
            "  inflating: dependency_treebank/wsj_0087.dp  \n",
            "  inflating: dependency_treebank/wsj_0157.dp  \n",
            "  inflating: dependency_treebank/wsj_0083.dp  \n",
            "  inflating: dependency_treebank/wsj_0103.dp  \n",
            "  inflating: dependency_treebank/wsj_0058.dp  \n",
            "  inflating: dependency_treebank/wsj_0054.dp  \n",
            "  inflating: dependency_treebank/wsj_0016.dp  \n",
            "  inflating: dependency_treebank/wsj_0126.dp  \n",
            "  inflating: dependency_treebank/wsj_0198.dp  \n",
            "  inflating: dependency_treebank/wsj_0144.dp  \n",
            "  inflating: dependency_treebank/wsj_0096.dp  \n",
            "  inflating: dependency_treebank/wsj_0086.dp  \n",
            "  inflating: dependency_treebank/wsj_0197.dp  \n",
            "  inflating: dependency_treebank/wsj_0025.dp  \n",
            "  inflating: dependency_treebank/wsj_0100.dp  \n",
            "  inflating: dependency_treebank/wsj_0084.dp  \n",
            "  inflating: dependency_treebank/wsj_0098.dp  \n",
            "  inflating: dependency_treebank/wsj_0106.dp  \n",
            "  inflating: dependency_treebank/wsj_0119.dp  \n",
            "  inflating: dependency_treebank/wsj_0092.dp  \n",
            "  inflating: dependency_treebank/wsj_0134.dp  \n",
            "  inflating: dependency_treebank/wsj_0077.dp  \n",
            "  inflating: dependency_treebank/wsj_0060.dp  \n",
            "  inflating: dependency_treebank/wsj_0172.dp  \n",
            "  inflating: dependency_treebank/wsj_0048.dp  \n",
            "  inflating: dependency_treebank/wsj_0030.dp  \n",
            "  inflating: dependency_treebank/wsj_0192.dp  \n",
            "  inflating: dependency_treebank/wsj_0066.dp  \n",
            "  inflating: dependency_treebank/wsj_0045.dp  \n",
            "  inflating: dependency_treebank/wsj_0155.dp  \n",
            "  inflating: dependency_treebank/wsj_0118.dp  \n",
            "  inflating: dependency_treebank/wsj_0152.dp  \n",
            "  inflating: dependency_treebank/wsj_0012.dp  \n",
            "  inflating: dependency_treebank/wsj_0006.dp  \n",
            "  inflating: dependency_treebank/wsj_0159.dp  \n",
            "  inflating: dependency_treebank/wsj_0163.dp  \n",
            "  inflating: dependency_treebank/wsj_0170.dp  \n",
            "  inflating: dependency_treebank/wsj_0141.dp  \n",
            "  inflating: dependency_treebank/wsj_0117.dp  \n",
            "  inflating: dependency_treebank/wsj_0125.dp  \n",
            "  inflating: dependency_treebank/wsj_0094.dp  \n",
            "  inflating: dependency_treebank/wsj_0169.dp  \n",
            "  inflating: dependency_treebank/wsj_0027.dp  \n",
            "  inflating: dependency_treebank/wsj_0010.dp  \n",
            "  inflating: dependency_treebank/wsj_0162.dp  \n",
            "  inflating: dependency_treebank/wsj_0127.dp  \n",
            "  inflating: dependency_treebank/wsj_0142.dp  \n",
            "  inflating: dependency_treebank/wsj_0046.dp  \n",
            "  inflating: dependency_treebank/wsj_0088.dp  \n",
            "  inflating: dependency_treebank/wsj_0079.dp  \n",
            "  inflating: dependency_treebank/wsj_0174.dp  \n",
            "  inflating: dependency_treebank/wsj_0063.dp  \n",
            "  inflating: dependency_treebank/wsj_0023.dp  \n",
            "  inflating: dependency_treebank/wsj_0004.dp  \n",
            "  inflating: dependency_treebank/wsj_0156.dp  \n",
            "  inflating: dependency_treebank/wsj_0133.dp  \n",
            "  inflating: dependency_treebank/wsj_0032.dp  \n",
            "  inflating: dependency_treebank/wsj_0070.dp  \n",
            "  inflating: dependency_treebank/wsj_0154.dp  \n",
            "  inflating: dependency_treebank/wsj_0095.dp  \n",
            "  inflating: dependency_treebank/wsj_0072.dp  \n",
            "  inflating: dependency_treebank/wsj_0183.dp  \n",
            "  inflating: dependency_treebank/wsj_0081.dp  \n",
            "  inflating: dependency_treebank/wsj_0196.dp  \n",
            "  inflating: dependency_treebank/wsj_0062.dp  \n",
            "  inflating: dependency_treebank/wsj_0124.dp  \n",
            "  inflating: dependency_treebank/wsj_0191.dp  \n",
            "  inflating: dependency_treebank/wsj_0013.dp  \n",
            "  inflating: dependency_treebank/wsj_0078.dp  \n",
            "  inflating: dependency_treebank/wsj_0150.dp  \n",
            "  inflating: dependency_treebank/wsj_0049.dp  \n",
            "  inflating: dependency_treebank/wsj_0189.dp  \n",
            "  inflating: dependency_treebank/wsj_0151.dp  \n",
            "  inflating: dependency_treebank/wsj_0193.dp  \n",
            "  inflating: dependency_treebank/wsj_0067.dp  \n",
            "  inflating: dependency_treebank/wsj_0145.dp  \n",
            "  inflating: dependency_treebank/wsj_0139.dp  \n",
            "  inflating: dependency_treebank/wsj_0166.dp  \n",
            "  inflating: dependency_treebank/wsj_0053.dp  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2r6z__jpAPq",
        "outputId": "e8370ea1-515a-44e6-ec3e-96db85607638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "vnAjuwg4kSgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir train_set\n",
        "!mkdir val_set\n",
        "!mkdir test_set"
      ],
      "metadata": {
        "id": "ggavZHbcTUas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d47675f-fc17-418e-97f3-b7c84df48924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘train_set’: File exists\n",
            "mkdir: cannot create directory ‘val_set’: File exists\n",
            "mkdir: cannot create directory ‘test_set’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
      ],
      "metadata": {
        "id": "tFUSc77sYi6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move first 100 text file to train\n",
        "for i in range(1, 101):\n",
        "  file_name = \"wsj_\" + (\"0\" * (4 - len(str(i)))) + str(i) + \".dp\"\n",
        "  src_name = os.path.join('/content/dependency_treebank/' + file_name)\n",
        "  trg_name = os.path.join('/content/train_set/' + file_name)\n",
        "  shutil.move(src_name, trg_name)\n",
        "\n",
        "# Move 101-150 text file to val\n",
        "for i in range(101, 151):\n",
        "  file_name = \"wsj_\" + (\"0\" * (4 - len(str(i)))) + str(i) + \".dp\"\n",
        "  src_name = os.path.join('/content/dependency_treebank/' + file_name)\n",
        "  trg_name = os.path.join('/content/val_set/' + file_name)\n",
        "  shutil.move(src_name, trg_name)\n",
        "\n",
        "# Move 151-199 text file to test\n",
        "for i in range(151, 200):\n",
        "  file_name = \"wsj_\" + (\"0\" * (4 - len(str(i)))) + str(i) + \".dp\"\n",
        "  src_name = os.path.join('/content/dependency_treebank/' + file_name)\n",
        "  trg_name = os.path.join('/content/test_set/' + file_name)\n",
        "  shutil.move(src_name, trg_name)\n",
        "\n",
        "!rm -rf \"/content/dependency_treebank\""
      ],
      "metadata": {
        "id": "OAdm2Wechnfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataframe(dataset_path):\n",
        "  dataframe_x_rows =  []\n",
        "  dataframe_y_rows = []\n",
        "\n",
        "  sentence_x_row = []\n",
        "  sentence_y_row = []\n",
        "\n",
        "  words = set([])\n",
        "  tags = set([])\n",
        "\n",
        "  for file_name in os.listdir(dataset_path):\n",
        "    file_path = dataset_path + \"/\" + file_name\n",
        "    file_number = file_name.split(\".\")[0].split(\"_\")[1]\n",
        "\n",
        "    with open(file_path, mode='r', encoding='utf-8') as file_text:\n",
        "      lines = file_text.readlines()\n",
        "\n",
        "      for line in lines:\n",
        "        split_line = line.split(\"\\t\")\n",
        "        if len(split_line) > 1:\n",
        "          word = split_line[0].lower()\n",
        "          pos_tag = split_line[1]\n",
        "\n",
        "          words.add(word)\n",
        "          tags.add(pos_tag)\n",
        "\n",
        "          sentence_x_row.append(word)\n",
        "          sentence_y_row.append(pos_tag)\n",
        "\n",
        "          if word == '.' or word == ';':\n",
        "            dataframe_x_rows.append(sentence_x_row)\n",
        "            dataframe_y_rows.append(sentence_y_row)\n",
        "            sentence_x_row = []\n",
        "            sentence_y_row = []\n",
        "\n",
        "  return dataframe_x_rows, dataframe_y_rows, words, tags"
      ],
      "metadata": {
        "id": "VWiOBcY_bsNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_x, df_train_y, train_words, train_tags = create_dataframe(\"/content/train_set\")"
      ],
      "metadata": {
        "id": "23_GbPw6cBsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_val_x, df_val_y, _, _ = create_dataframe(\"/content/val_set\")"
      ],
      "metadata": {
        "id": "A7NQ3-kJ3QBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_x, df_test_y, _, _ = create_dataframe(\"/content/test_set\")"
      ],
      "metadata": {
        "id": "wQDqjYB_PlrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2index =  {w: i + 2 for i, w in enumerate(list(train_words))}\n",
        "word2index['<PAD>'] = 0\n",
        "word2index['<OOV>'] = 1\n",
        "\n",
        "tag2index = {t: i for i, t in enumerate(list(train_tags))}"
      ],
      "metadata": {
        "id": "SRQKdlcQ8sPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences_X, val_sentences_X, train_tags_y, val_tags_y = [], [], [], []\n",
        "\n",
        "for sentence in df_train_x:\n",
        "  converted_sentence = []\n",
        "  for word in sentence:\n",
        "    try:\n",
        "      converted_sentence.append(word2index[word])\n",
        "    except KeyError:\n",
        "      converted_sentence.append(word2index[\"<OOV>\"])\n",
        "  train_sentences_X.append(converted_sentence)\n",
        "\n",
        "for sentence in df_val_x:\n",
        "  converted_sentence = []\n",
        "  for word in sentence:\n",
        "    try:\n",
        "      converted_sentence.append(word2index[word])\n",
        "    except KeyError:\n",
        "      converted_sentence.append(word2index[\"<OOV>\"])\n",
        "  val_sentences_X.append(converted_sentence)"
      ],
      "metadata": {
        "id": "APguw-xF6h0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = OneHotEncoder()\n",
        "encoder.fit(np.array(list(train_tags)).reshape(-1, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYk2JV0-slTL",
        "outputId": "8bf4cb70-6b98-4f67-b81b-3bed55a92d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneHotEncoder()"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
        "\n",
        "print(MAX_LENGTH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6Z6CFe2Ad3w",
        "outputId": "c3243be1-1854-4020-98bc-6ffb728c1b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tags_y = [np.array([tag2index[t] for t in s] + [tag2index[s[-1]] for i in range(0,MAX_LENGTH-len(s))]) for s in df_train_y]\n",
        "val_tags_y =[np.array([tag2index[t] for t in s[:MAX_LENGTH]] + [tag2index[s[-1]] for i in range(0,MAX_LENGTH-len(s))]) for s in df_val_y]\n",
        "test_tags_y =[np.array([tag2index[t] for t in s[:MAX_LENGTH]] + [tag2index[s[-1]] for i in range(0,MAX_LENGTH-len(s))]) for s in df_test_y]"
      ],
      "metadata": {
        "id": "4u6LICx2IZBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "val_sentences_X = pad_sequences(val_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "train_tags_y = np.asarray(train_tags_y)\n",
        "val_tags_y = np.asarray(val_tags_y)\n",
        "test_tags_y = np.asarray(test_tags_y)"
      ],
      "metadata": {
        "id": "A2me5GaZAwC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dimension = 100\n",
        "embedding_model = gloader.load(\"glove-wiki-gigaword-100\")"
      ],
      "metadata": {
        "id": "FQSplRwWImUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(train_words) + 2\n",
        "\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dimension))\n",
        "for word, idx in word2index.items():\n",
        "  try:\n",
        "    embedding_vector = embedding_model[word]\n",
        "  except (KeyError, TypeError):\n",
        "    embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "  embedding_matrix[idx] = embedding_vector"
      ],
      "metadata": {
        "id": "pUlRxvgPDyS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tag, tag_value in tag2index.items():\n",
        "  print(str(tag_value) + \" \" + tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh9fvkdjaaQG",
        "outputId": "76ea43d4-5527-4255-bf47-68078054ba55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 PRP$\n",
            "1 NNS\n",
            "2 WP$\n",
            "3 JJR\n",
            "4 JJ\n",
            "5 VB\n",
            "6 NNPS\n",
            "7 RBS\n",
            "8 DT\n",
            "9 NN\n",
            "10 WP\n",
            "11 VBZ\n",
            "12 :\n",
            "13 $\n",
            "14 NNP\n",
            "15 TO\n",
            "16 CC\n",
            "17 -LRB-\n",
            "18 LS\n",
            "19 ''\n",
            "20 -RRB-\n",
            "21 PRP\n",
            "22 SYM\n",
            "23 EX\n",
            "24 #\n",
            "25 WDT\n",
            "26 IN\n",
            "27 RP\n",
            "28 CD\n",
            "29 ,\n",
            "30 WRB\n",
            "31 VBD\n",
            "32 UH\n",
            "33 FW\n",
            "34 POS\n",
            "35 .\n",
            "36 JJS\n",
            "37 VBP\n",
            "38 VBN\n",
            "39 PDT\n",
            "40 RB\n",
            "41 RBR\n",
            "42 VBG\n",
            "43 ``\n",
            "44 MD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline model"
      ],
      "metadata": {
        "id": "p91SaJgsndpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logits_to_tokens(sequences, index):\n",
        "    token_sequences = []\n",
        "    for categorical_sequence in sequences:\n",
        "        token_sequence = []\n",
        "        for categorical in categorical_sequence:\n",
        "            token_sequence.append(index[np.argmax(categorical)])\n",
        " \n",
        "        token_sequences.append(token_sequence)\n",
        " \n",
        "    return token_sequences"
      ],
      "metadata": {
        "id": "Sdkn_G2Gbwhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, TimeDistributed, Activation\n",
        "from keras.models import Model\n",
        "\n",
        "def build_model(num_tokens, embedding_dimension, embedding_matrix):\n",
        "    input = Input(shape=(None,), dtype=\"int32\")\n",
        "    x = Embedding(num_tokens,\n",
        "                  embedding_dimension,\n",
        "                  embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "                  trainable=False,\n",
        "                  mask_zero=True)(input)\n",
        "    x = Bidirectional(LSTM(100, return_sequences=True))(x)\n",
        "    x = TimeDistributed(Dense(len(tag2index), activation=\"softmax\"))(x)\n",
        "    return Model(input, x)\n",
        "\n",
        " \n",
        "print(num_tokens)\n",
        "print(embedding_dimension)\n",
        "model = build_model(num_tokens, embedding_dimension, embedding_matrix)\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVJdOa7DMJ6s",
        "outputId": "37d79c7c-1f23-46b7-fd46-c46d52a00607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7406\n",
            "100\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, None, 100)         740600    \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, None, 200)        160800    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDis  (None, None, 45)         9045      \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 910,445\n",
            "Trainable params: 169,845\n",
            "Non-trainable params: 740,600\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from functools import partial\n",
        "import random\n",
        "\n",
        "def show_history(history: tf.keras.callbacks.History):\n",
        "    \"\"\"\n",
        "    Shows training history data stored by the History Keras callback\n",
        "\n",
        "    :param history: History Keras callback\n",
        "    \"\"\"\n",
        "    history_data = history.history\n",
        "\n",
        "    for key, value in history_data.items():\n",
        "        if not key.startswith('val'):\n",
        "            fig, ax = plt.subplots(1, 1)\n",
        "            ax.set_title(key)\n",
        "            ax.plot(value)\n",
        "            if f'val_{key}' in history_data:\n",
        "                ax.plot(history_data[f'val_{key}'])\n",
        "            else:\n",
        "                print(f\"Couldn't find validation values for metric: {key}\")\n",
        "\n",
        "            ax.set_ylabel(key)\n",
        "            ax.set_xlabel('epoch')\n",
        "            ax.legend(['train', 'val'], loc='best')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sWhy_MOd1eIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)"
      ],
      "metadata": {
        "id": "CmQS_VDfKFdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences_X = []\n",
        "\n",
        "for sentence in df_test_x:\n",
        "  converted_sentence = []\n",
        "  for word in sentence:\n",
        "    try:\n",
        "      converted_sentence.append(word2index[word])\n",
        "    except KeyError:\n",
        "      converted_sentence.append(word2index[\"<OOV>\"])\n",
        "  test_sentences_X.append(converted_sentence)\n",
        "  \n",
        "\n",
        "\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n"
      ],
      "metadata": {
        "id": "D3V1so41PkHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_info = {\n",
        "    'verbose': 1,\n",
        "    'epochs': 100,\n",
        "    'batch_size': 64,\n",
        "}\n",
        "\n",
        "pat = 5 #setting the patience value\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=pat, min_delta=1e-4, restore_best_weights=True)\n",
        "\n",
        "print(f\"Start training! \\nParameters: {training_info}\")\n",
        "history = model.fit(x=train_sentences_X,\n",
        "                    y=to_categorical(train_tags_y, len(tag2index)),\n",
        "                    validation_data=(val_sentences_X, \n",
        "                                     to_categorical(val_tags_y, len(tag2index))),\n",
        "                    callbacks=[es],\n",
        "                    **training_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvumXGpSoQNd",
        "outputId": "f22849ae-f15d-4a86-e4fc-e53f794e4427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training! \n",
            "Parameters: {'verbose': 1, 'epochs': 100, 'batch_size': 64}\n",
            "Epoch 1/100\n",
            "32/32 [==============================] - 12s 120ms/step - loss: 1.0187 - accuracy: 0.2845 - val_loss: 0.8500 - val_accuracy: 0.4386\n",
            "Epoch 2/100\n",
            "32/32 [==============================] - 1s 30ms/step - loss: 0.6803 - accuracy: 0.5265 - val_loss: 0.5726 - val_accuracy: 0.5821\n",
            "Epoch 3/100\n",
            "32/32 [==============================] - 1s 33ms/step - loss: 0.4371 - accuracy: 0.6997 - val_loss: 0.4131 - val_accuracy: 0.6969\n",
            "Epoch 4/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.3118 - accuracy: 0.7833 - val_loss: 0.3370 - val_accuracy: 0.7502\n",
            "Epoch 5/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.2485 - accuracy: 0.8229 - val_loss: 0.2967 - val_accuracy: 0.7791\n",
            "Epoch 6/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.2113 - accuracy: 0.8475 - val_loss: 0.2714 - val_accuracy: 0.7969\n",
            "Epoch 7/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.1857 - accuracy: 0.8655 - val_loss: 0.2538 - val_accuracy: 0.8097\n",
            "Epoch 8/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.1672 - accuracy: 0.8766 - val_loss: 0.2387 - val_accuracy: 0.8192\n",
            "Epoch 9/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.1532 - accuracy: 0.8844 - val_loss: 0.2288 - val_accuracy: 0.8259\n",
            "Epoch 10/100\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.1417 - accuracy: 0.8930 - val_loss: 0.2246 - val_accuracy: 0.8277\n",
            "Epoch 11/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.1322 - accuracy: 0.8986 - val_loss: 0.2145 - val_accuracy: 0.8364\n",
            "Epoch 12/100\n",
            "32/32 [==============================] - 1s 25ms/step - loss: 0.1243 - accuracy: 0.9060 - val_loss: 0.2157 - val_accuracy: 0.8335\n",
            "Epoch 13/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.1169 - accuracy: 0.9109 - val_loss: 0.2042 - val_accuracy: 0.8446\n",
            "Epoch 14/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.1110 - accuracy: 0.9150 - val_loss: 0.2028 - val_accuracy: 0.8477\n",
            "Epoch 15/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.1046 - accuracy: 0.9209 - val_loss: 0.1991 - val_accuracy: 0.8503\n",
            "Epoch 16/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.0994 - accuracy: 0.9256 - val_loss: 0.1978 - val_accuracy: 0.8510\n",
            "Epoch 17/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.0950 - accuracy: 0.9281 - val_loss: 0.1933 - val_accuracy: 0.8554\n",
            "Epoch 18/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.0905 - accuracy: 0.9321 - val_loss: 0.1918 - val_accuracy: 0.8562\n",
            "Epoch 19/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.0865 - accuracy: 0.9349 - val_loss: 0.1931 - val_accuracy: 0.8547\n",
            "Epoch 20/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.0833 - accuracy: 0.9378 - val_loss: 0.1886 - val_accuracy: 0.8581\n",
            "Epoch 21/100\n",
            "32/32 [==============================] - 1s 25ms/step - loss: 0.0796 - accuracy: 0.9402 - val_loss: 0.1878 - val_accuracy: 0.8590\n",
            "Epoch 22/100\n",
            "32/32 [==============================] - 1s 41ms/step - loss: 0.0758 - accuracy: 0.9434 - val_loss: 0.1865 - val_accuracy: 0.8599\n",
            "Epoch 23/100\n",
            "32/32 [==============================] - 1s 37ms/step - loss: 0.0727 - accuracy: 0.9462 - val_loss: 0.1890 - val_accuracy: 0.8591\n",
            "Epoch 24/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0695 - accuracy: 0.9489 - val_loss: 0.1854 - val_accuracy: 0.8616\n",
            "Epoch 25/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0664 - accuracy: 0.9511 - val_loss: 0.1842 - val_accuracy: 0.8621\n",
            "Epoch 26/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0636 - accuracy: 0.9536 - val_loss: 0.1899 - val_accuracy: 0.8596\n",
            "Epoch 27/100\n",
            "32/32 [==============================] - 1s 26ms/step - loss: 0.0607 - accuracy: 0.9565 - val_loss: 0.1850 - val_accuracy: 0.8631\n",
            "Epoch 28/100\n",
            "32/32 [==============================] - 1s 26ms/step - loss: 0.0582 - accuracy: 0.9581 - val_loss: 0.1830 - val_accuracy: 0.8637\n",
            "Epoch 29/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.0556 - accuracy: 0.9605 - val_loss: 0.1824 - val_accuracy: 0.8646\n",
            "Epoch 30/100\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.0532 - accuracy: 0.9632 - val_loss: 0.1844 - val_accuracy: 0.8636\n",
            "Epoch 31/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.0508 - accuracy: 0.9645 - val_loss: 0.1895 - val_accuracy: 0.8619\n",
            "Epoch 32/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.0484 - accuracy: 0.9671 - val_loss: 0.1838 - val_accuracy: 0.8646\n",
            "Epoch 33/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.0463 - accuracy: 0.9693 - val_loss: 0.1854 - val_accuracy: 0.8645\n",
            "Epoch 34/100\n",
            "32/32 [==============================] - 1s 24ms/step - loss: 0.0444 - accuracy: 0.9706 - val_loss: 0.1898 - val_accuracy: 0.8626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_history(history)"
      ],
      "metadata": {
        "id": "eBAho2GhoTzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))"
      ],
      "metadata": {
        "id": "SIAGQxAWPffG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scores)"
      ],
      "metadata": {
        "id": "bBEP4Uhpe4Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "flatten_y = [word for sentence in df_train_y for word in sentence]\n",
        "\n",
        "plt.figure(figsize=(30, 15))\n",
        "plt.hist(flatten_y, bins=\"auto\", log=True, rwidth=0.7);"
      ],
      "metadata": {
        "id": "zEyoja7Mhkb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [i for i in range(1, 46) if i not in [11, 15, 18, 25, 30, 35, 37]]\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "8Yink-tMrki0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C6L5jZ_MvhT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "pred = model.predict(test_sentences_X)\n",
        "\n",
        "print(pred.shape)\n",
        "#y_test_decoded = encoder.inverse_transform([test_tags_y])\n",
        "#y_pred_decoded = encoder.inverse_transofrm(convert_labels(pred))\n",
        "\n",
        "#print(classification_report(y_test_decoded, y_pred_decoded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9J8jKLWp3vJ",
        "outputId": "319a1f28-214e-4ff8-c23e-a7a68d798fa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 3s 10ms/step\n",
            "(653, 67, 45)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_labels(e):\n",
        "  # set the highest value to 1\n",
        "  e[np.argmax(e)]=1\n",
        "  # set the rest to 0\n",
        "  e[e!=1]=0\n",
        "  return e\n",
        "\n",
        "def reduce_results(test, y_true, y_pred):\n",
        "  verylonglist_pred = []\n",
        "  verylonglist_true = []\n",
        "  j = 0\n",
        "  for e in test:\n",
        "    maxlen = min(len(e), MAX_LENGTH)\n",
        "    for i in range(0,maxlen):\n",
        "      verylonglist_pred.append(convert_labels(y_pred[j][i]))\n",
        "      verylonglist_true.append(y_true[j][i])\n",
        "    j = j + 1 \n",
        "  return verylonglist_true, verylonglist_pred\n",
        "\n",
        "true_y, pred_y = reduce_results(df_test_x,to_categorical(test_tags_y, len(tag2index)), pred )\n"
      ],
      "metadata": {
        "id": "EY3PKKF-xUFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "395cde98-33d0-4ae6-cdb4-f5ece4f4a37d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(true_y,pred_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6StjUW8Op0h",
        "outputId": "2e2a1382-e311-46c5-f3a3-d5512da4ee9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98        99\n",
            "           1       0.82      0.81      0.81       941\n",
            "           2       0.00      0.00      0.00         4\n",
            "           3       0.86      0.61      0.71        59\n",
            "           4       0.67      0.74      0.70       918\n",
            "           5       0.90      0.95      0.92       402\n",
            "           6       0.00      0.00      0.00        44\n",
            "           7       1.00      0.67      0.80         3\n",
            "           8       0.98      0.99      0.98      1334\n",
            "           9       0.85      0.81      0.83      2381\n",
            "          10       1.00      1.00      1.00        20\n",
            "          11       0.96      0.91      0.93       280\n",
            "          12       1.00      1.00      1.00        56\n",
            "          13       1.00      1.00      1.00       209\n",
            "          14       0.74      0.86      0.79      1504\n",
            "          15       0.99      0.99      0.99       386\n",
            "          16       0.99      0.99      0.99       366\n",
            "          17       0.26      0.61      0.37        18\n",
            "          18       0.00      0.00      0.00         0\n",
            "          19       0.99      0.99      0.99        70\n",
            "          20       0.26      0.50      0.35        18\n",
            "          21       0.98      0.97      0.98       192\n",
            "          22       0.00      0.00      0.00         0\n",
            "          23       1.00      1.00      1.00         5\n",
            "          24       0.00      0.00      0.00         0\n",
            "          25       0.93      0.95      0.94        84\n",
            "          26       0.96      0.96      0.96      1630\n",
            "          27       0.59      0.70      0.64        33\n",
            "          28       0.97      0.90      0.93       858\n",
            "          29       1.00      1.00      1.00       787\n",
            "          30       1.00      0.88      0.93        24\n",
            "          31       0.90      0.85      0.87       634\n",
            "          32       0.00      0.00      0.00         0\n",
            "          33       0.00      0.00      0.00         0\n",
            "          34       0.99      0.99      0.99       152\n",
            "          35       1.00      1.00      1.00       637\n",
            "          36       0.92      0.74      0.82        31\n",
            "          37       0.91      0.82      0.86       134\n",
            "          38       0.76      0.72      0.74       366\n",
            "          39       0.00      0.00      0.00         4\n",
            "          40       0.79      0.76      0.77       381\n",
            "          41       0.39      0.47      0.42        15\n",
            "          42       0.79      0.57      0.67       221\n",
            "          43       1.00      0.99      0.99        73\n",
            "          44       0.96      0.99      0.98       167\n",
            "\n",
            "   micro avg       0.88      0.88      0.88     15540\n",
            "   macro avg       0.71      0.70      0.70     15540\n",
            "weighted avg       0.88      0.88      0.88     15540\n",
            " samples avg       0.88      0.88      0.88     15540\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}